{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lighthouse Labs - Synaptive Medical\n",
    "### Introduction to Machine Learning\n",
    "### W3D1 part 2 Classification Models: Logistic Regression and Random Forests\n",
    "\n",
    "Instructor: Socorro Dominguez  \n",
    "December 04, 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda:**\n",
    "1. Regression problems Vs. classification problems\n",
    "\n",
    "\n",
    "\n",
    "2. Logistic regression\n",
    "   * Uses of logistic regression\n",
    "   * Why is it called regression?\n",
    "\n",
    "\n",
    "\n",
    "3. Tree Based Algorithms\n",
    "    * Decision Trees\n",
    "    * Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from display_tree import display_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# pip install git+git://github.com/mgelbart/plot-classifier.git\n",
    "from plot_classifier import plot_classifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tree(X, y, model, predict_proba = False):\n",
    "    \n",
    "    # Join data for plotting\n",
    "    sample = (X.join(y))\n",
    "    # Create a mesh for plotting\n",
    "    step = (X.max() - X.min()) / 50\n",
    "    x1, x2 = np.meshgrid(np.arange(sample.min()[0]-step[0], sample.max()[0]+step[0], step[0]),\n",
    "                         np.arange(sample.min()[1]-step[1], sample.max()[1]+step[1], step[1]))\n",
    "\n",
    "    # Store mesh in dataframe\n",
    "    mesh_df = pd.DataFrame(np.c_[x1.ravel(), x2.ravel()], columns=['x1', 'x2'])\n",
    "\n",
    "    # Mesh predictions\n",
    "    if predict_proba:\n",
    "        mesh_df['predictions'] = model.predict_proba(mesh_df[['x1', 'x2']])[:, 0]\n",
    "        # Plot\n",
    "        base_plot = alt.Chart(mesh_df).mark_rect(opacity=0.5).encode(\n",
    "            x=alt.X('x1', bin=alt.Bin(step=step[0])),\n",
    "            y=alt.Y('x2', bin=alt.Bin(step=step[1])),\n",
    "            color=alt.Color('predictions', title='P(blue)', scale=alt.Scale(scheme='redblue'))\n",
    "        ).properties(\n",
    "            width=400,\n",
    "            height=400\n",
    "        )\n",
    "        return alt.layer(base_plot).configure_axis(\n",
    "            labelFontSize=20,\n",
    "            titleFontSize=20\n",
    "        ).configure_legend(\n",
    "            titleFontSize=20,\n",
    "            labelFontSize=20\n",
    "        )\n",
    "    else:\n",
    "        mesh_df['predictions'] = model.predict(mesh_df[['x1', 'x2']])\n",
    "        # Plot\n",
    "        scat_plot = alt.Chart(sample).mark_circle(\n",
    "            stroke='black',\n",
    "            opacity=1,\n",
    "            strokeWidth=1.5,\n",
    "            size=100\n",
    "        ).encode(\n",
    "            x=alt.X(X.columns[0], axis=alt.Axis(labels=True, ticks=True, title=X.columns[0])),\n",
    "            y=alt.Y(X.columns[1], axis=alt.Axis(labels=True, ticks=True, title=X.columns[1])),\n",
    "            color=alt.Color(y.columns[0])\n",
    "        )\n",
    "        base_plot = alt.Chart(mesh_df).mark_rect(opacity=0.5).encode(\n",
    "            x=alt.X('x1', bin=alt.Bin(step=step[0])),\n",
    "            y=alt.Y('x2', bin=alt.Bin(step=step[1])),\n",
    "            color=alt.Color('predictions', title='Legend')\n",
    "        ).properties(\n",
    "            width=400,\n",
    "            height=400\n",
    "        )\n",
    "        return alt.layer(base_plot, scat_plot).configure_axis(\n",
    "            labelFontSize=20,\n",
    "            titleFontSize=20\n",
    "        ).configure_legend(\n",
    "            titleFontSize=20,\n",
    "            labelFontSize=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a specific feature is examined. If the value of the feature is below a specific threshold, then the left branch is followed; otherwise, the right branch is followed. \n",
    "As the leaf node is reached, the decision is made about the class to which the example belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Decision Tree Building**\n",
    "- Get our data in\n",
    "- Train / validation split\n",
    "- Fit a **classification tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Case study:** Suppose you have three different job offers with comparable salaries and job descriptions. You want to decide which one to accept, and you want to make this decision based on which job is likely to make you happy. Being a very systematic person, you come up with three features associated with the offers, which are important for your happiness: whether the colleagues are supportive, work-hour flexibility, and whether the company is a start-up or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supportive_colleagues</th>\n",
       "      <th>work_hour_flexibility</th>\n",
       "      <th>start_up</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   supportive_colleagues  work_hour_flexibility  start_up target\n",
       "0                      1                      0         0      ?\n",
       "1                      0                      0         1      ?\n",
       "2                      0                      1         1      ?"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offer_data = {\n",
    "    # Features\n",
    "    \"supportive_colleagues\": [1, 0, 0],\n",
    "    \"work_hour_flexibility\": [0, 0, 1],\n",
    "    \"start_up\": [0, 1, 1],\n",
    "    # Target\n",
    "    \"target\": [\"?\", \"?\", \"?\"],\n",
    "}\n",
    "\n",
    "offer_df = pd.DataFrame(offer_data)\n",
    "offer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, you ask the following questions to some of your friends (who you think have similar notions of happiness) regarding their jobs:\n",
    "\n",
    "Do you have supportive colleagues? (1 for 'yes' and 0 for 'no')\n",
    "Do you have flexible work hours? (1 for 'yes' and 0 for 'no')\n",
    "Do you work for a start-up? (1 for 'start up' and 0 for 'non start up')\n",
    "Are you happy in your job? (happy or unhappy)\n",
    "You get the following data from this toy survey. Your goal is to train a machine learning model using this toy data and then use this model to predict which job is likely to make you happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supportive_colleagues</th>\n",
       "      <th>work_hour_flexibility</th>\n",
       "      <th>start_up</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   supportive_colleagues  work_hour_flexibility  start_up   target\n",
       "0                      1                      1         1    happy\n",
       "1                      1                      1         0    happy\n",
       "2                      1                      0         1    happy\n",
       "3                      0                      1         0  unhappy\n",
       "4                      0                      1         1  unhappy\n",
       "5                      1                      0         0    happy\n",
       "6                      1                      1         0    happy\n",
       "7                      0                      0         1  unhappy\n",
       "8                      1                      0         1  unhappy\n",
       "9                      0                      0         0  unhappy"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_data = {\n",
    "    # Features\n",
    "    \"supportive_colleagues\": [1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
    "    \"work_hour_flexibility\": [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "    \"start_up\": [1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "    # Target\n",
    "    \"target\": [\n",
    "        \"happy\",\n",
    "        \"happy\",\n",
    "        \"happy\",\n",
    "        \"unhappy\",\n",
    "        \"unhappy\",\n",
    "        \"happy\",\n",
    "        \"happy\",\n",
    "        \"unhappy\",\n",
    "        \"unhappy\",\n",
    "        \"unhappy\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "train_df = pd.DataFrame(happiness_data)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea of a machine learning algorithm is to fit the best model on the given training data, which is in the form of feature vectors (X) and their corresponding targets(y), and then using this model to predict targets for new examples (represented with feature vectors).\n",
    "\n",
    "From train_df, create the feature table and save it in an object named X and the target in an object named y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[\"target\"])\n",
    "y = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_tree = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# instantiate a class of the DecisionTreeClassifier\n",
    "toy_tree = DecisionTreeClassifier()\n",
    "# fit the model to the data. The semicolon at the end is used to suppress displaying the output of model.fit\n",
    "toy_tree.fit(X, y)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.1 (20200629.0846)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"323pt\" height=\"266pt\"\n",
       " viewBox=\"0.00 0.00 322.50 266.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 262)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-262 318.5,-262 318.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"221,-258 42,-258 42,-220 221,-220 221,-258\"/>\n",
       "<text text-anchor=\"middle\" x=\"131.5\" y=\"-242.8\" font-family=\"Times,serif\" font-size=\"14.00\">supportive_colleagues &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"131.5\" y=\"-227.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = happy</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"105,-183 0,-183 0,-147 105,-147 105,-183\"/>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-161.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = unhappy</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M111.56,-219.83C101.63,-210.78 89.5,-199.72 78.81,-189.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.09,-187.32 71.34,-183.17 76.37,-192.49 81.09,-187.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"72.5\" y=\"-204.44\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"299.5,-184 123.5,-184 123.5,-146 299.5,-146 299.5,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"211.5\" y=\"-168.8\" font-family=\"Times,serif\" font-size=\"14.00\">work_hour_flexibility &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"211.5\" y=\"-153.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = happy</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.69,-219.83C161.35,-211.13 173.08,-200.58 183.58,-191.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"186.23,-193.45 191.32,-184.16 181.54,-188.25 186.23,-193.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.01\" y=\"-205.43\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"204,-110 103,-110 103,-72 204,-72 204,-110\"/>\n",
       "<text text-anchor=\"middle\" x=\"153.5\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">start_up &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"153.5\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = happy</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.86,-145.83C190.18,-137.54 182.14,-127.56 174.81,-118.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.33,-116.01 168.33,-110.41 171.88,-120.4 177.33,-116.01\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"314.5,-109 222.5,-109 222.5,-73 314.5,-73 314.5,-109\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.5\" y=\"-87.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = happy</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M225.88,-145.83C232.72,-137.2 241.01,-126.73 248.45,-117.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.25,-119.43 254.71,-109.42 245.76,-115.09 251.25,-119.43\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"144.5,-36 52.5,-36 52.5,0 144.5,0 144.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = happy</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.34,-71.72C132.81,-63.29 124.95,-53.15 117.88,-44.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.56,-41.77 111.67,-36 115.03,-46.05 120.56,-41.77\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"254.5,-36 162.5,-36 162.5,0 254.5,0 254.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = happy</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.66,-71.72C174.19,-63.29 182.05,-53.15 189.12,-44.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.97,-46.05 195.33,-36 186.44,-41.77 191.97,-46.05\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f881d0187d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_displayed = None\n",
    "### BEGIN SOLUTION\n",
    "toy_displayed = display_tree(X.columns, toy_tree)\n",
    "toy_displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supportive_colleagues</th>\n",
       "      <th>work_hour_flexibility</th>\n",
       "      <th>start_up</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   supportive_colleagues  work_hour_flexibility  start_up   target predicted\n",
       "0                      1                      1         1    happy     happy\n",
       "1                      1                      1         0    happy     happy\n",
       "2                      1                      0         1    happy     happy\n",
       "3                      0                      1         0  unhappy   unhappy\n",
       "4                      0                      1         1  unhappy   unhappy\n",
       "5                      1                      0         0    happy     happy\n",
       "6                      1                      1         0    happy     happy\n",
       "7                      0                      0         1  unhappy   unhappy\n",
       "8                      1                      0         1  unhappy     happy\n",
       "9                      0                      0         0  unhappy   unhappy"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_train = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "predicted_train = train_df.assign(predicted = toy_tree.predict(X))\n",
    "predicted_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supportive_colleagues</th>\n",
       "      <th>work_hour_flexibility</th>\n",
       "      <th>start_up</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>unhappy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   supportive_colleagues  work_hour_flexibility  start_up target predicted\n",
       "0                      1                      0         0      ?     happy\n",
       "1                      0                      0         1      ?   unhappy\n",
       "2                      0                      1         1      ?   unhappy"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offer_X = offer_df.drop(columns=[\"target\"])\n",
    "\n",
    "pred_offer_df = offer_df.assign(predicted = toy_tree.predict(offer_X))\n",
    "pred_offer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did the classification tree do?\n",
    "- Found a good way to split and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So how do we decide the split?\n",
    "- Basic idea is to pick a criterion (see [here](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation)) and then minimize it across possible splits.\n",
    "- Common one is Gini impurity\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "<img src='imgs/gini.png' width=300>\n",
    "\n",
    "- $C$ is number of classes in target variable\n",
    "- $p$ is proportion of class $i$ in a group\n",
    "- tells us what is the probability of misclassifying an observation --> lower the better\n",
    "\n",
    "\n",
    "Full details here: https://towardsdatascience.com/gini-impurity-measure-dbd3878ead33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees, pros and cons\n",
    "### Advantages:\n",
    "- Simple to understand and to interpret. Trees can be visualised.\n",
    "- Requires little data preparation. Doesn't require data normalisation.\n",
    "- Can handle multi-class classification well.\n",
    "\n",
    "### Disadvantages:\n",
    "- Tendency to overfit with overly complex trees that don't generalize well; pruning techniques (e.g., minimum number of samples required at a leaf node, maximum depth of tree) is needed to avoid the problem\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated; mitigated through ensembles (more on this soon)\n",
    "- Is a \"greedy\" algorithm. Each node is only locally optimal which cannot gaurantee globally optimal tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "### General idea:\n",
    "   - `fit` a diverse set of classifiers by **injecting randomness** in the classifier construction\n",
    "   - `predict` by taking the average of predictions given by individual classifiers\n",
    "\n",
    "### How do we inject randomness in the classifier construction? \n",
    "   1. Data: Build each tree on a bootstrap sample (i.e., a sample drawn **with replacement** from the training set)\n",
    "   2. Features: Consider a random subset of features at each split (`RandomForestClassifier`)\n",
    "        \n",
    "**The intuition here is the wisdom of crowds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The random forest classifier \n",
    "\n",
    "- Create a collection (ensemble) of trees. Grow each tree on an independent bootstrap sample from the data.\n",
    "- At each node:\n",
    "    - Randomly select a subset of features out of all features (independently for each node)\n",
    "    - Find the best split on the selected features\n",
    "    - Grow the trees to maximum depth\n",
    "- Vote between the trees to get predictions for new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forest Training\n",
    "- Showing feature subset\n",
    "\n",
    "<img src='imgs/random-forest-features.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forest Prediction\n",
    "<img src='imgs/random-forest-predict.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beating the fundamental tradeoff...\n",
    "\n",
    "- Decreasing training error and not increasing approximation error.\n",
    "- This is the promise of ensembles, though it's not guaranteed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests summary\n",
    "\n",
    "- Accuracy\n",
    "    - Usually more accurate compared to decision trees, usually one of the best performing off-the-shelf classifiers\n",
    "- Speed\n",
    "    - Slower than decision trees because we are fitting multiple trees \n",
    "    - But can easily parallelize training because all trees are independent of each other \n",
    "- Overfitting\n",
    "    - No depth limit decision tree tends to overfit \n",
    "    - Random forests are less likely to overfit\n",
    "- Interpretability\n",
    "    - Decision trees are more interpretable (can still use `feature_importance` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, f1_score, roc_auc_score\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier \n",
    "\n",
    "# Other\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from sklearn import datasets\n",
    "from plot_classifier import plot_classifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio # conda install -c plotly plotly-orca\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The first thing to say is that logistic regression is not a regression, but a classification learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Regression, we allow the response to take on any real number. But what if the range is restricted?\n",
    "\n",
    "1. Positive values: river flow. \n",
    "    - Lower limit: 0\n",
    "2. Percent/proportion data: proportion of income spent on housing in Vancouver. \n",
    "    - Lower limit: 0\n",
    "    - Upper limit: 1. \n",
    "3. Binary data: success/failure data.\n",
    "    - Only take values of 0 and 1.\n",
    "4. Count data: number of male crabs nearby a nesting female\n",
    "    - Only take count values (0, 1, 2, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we fit a regression curve to stay within the bounds of the data, while still retaining the interpretability that we have with a linear model function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Link Functions\n",
    "\n",
    "We can transform the model function. \n",
    "\n",
    "**Logistic Regression** It is a special case of **linear regression** where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- $\\text{logit}(x)=\\log(\\frac{x}{(1+e^{-(\\beta_0+\\beta_iX_i)})})$\n",
    "\n",
    "for binary response values.\n",
    "\n",
    "- Parameter interpretation: an increase of one unit in the predictor is associated with an $\\exp(\\beta)$ times increase in the odds of \"success\", where $\\beta$ is the slope parameter, and odds is the ratio of success to failure probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression's answer is based on Least Squared Squares. For fitting the best model in Logistic Regression,  **Maximum Likelihood Estimation** is used. \n",
    "\n",
    "Maximizing the likelihood function helps find the parameters that are most likely to produce the observed data. MLE sets the mean and variance as parameters and helps us find  the specific parametric values for a given model. This set of parameters can be used for predicting the data needed in a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function gives an ‘Sigmoid’ shaped curve that can take any real-valued number and map it into a value between 0 and 1. \n",
    "\n",
    "If the curve goes to positive infinity, the model will predict 1.\n",
    "If the model goes to negative infinity, the model will predict 0.\n",
    "\n",
    "If the output of the sigmoid function is more than 0.5, we usually classify the outcome as 1 otherwise, as 0. We can change this threshold.\n",
    "\n",
    "We can say that the Sigmoid Function is a special case of the Softmax function when we only have two classes. - More later but very important for NN-\n",
    "\n",
    "*You can check more about this (Odds and Log of Odss in the additional bonus slides)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why people use linear classifiers\n",
    "\n",
    "Logistic regression are used EVERYWHERE!\n",
    "\n",
    "- Fast training and testing.\n",
    "  - Training on huge datasets.\n",
    "  - Testing is just computing $w^Tx_i$.\n",
    "- Weights $w_j$ are easy to understand. INTERPRETABILITY\n",
    "  - It's how much $x_j$ changes the prediction and in what direction.\n",
    "- We can often get a good test error.\n",
    "  - With many features.\n",
    "- Smoother predictions than random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Bonus Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## GLM's (Generalized Linear Models)\n",
    "\n",
    "Since Logistic Regression has class 1s and 0s it is a binomial distribution and not a Normal Distribution\n",
    "\n",
    "GLM's use a link function to connect the linear model to a model that predicts for non normal distribution. As we saw earlier, our link function was the Sigmoid Function.\n",
    "\n",
    "Generalized Linear Model (GLM) is a generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. (Counts of male crabs around a nesting female?)\n",
    "\n",
    "When you fit a GLM, you already have the information needed to produce a probabilistic forecast. This is because, in addition to specifying the relationship between the predictors and the mean, we also specify a distribution of the response (such as Poisson, Bernoulli, or Gaussian).\n",
    "\n",
    "You will do a small exercise today on this. And it will be clearer.\n",
    "\n",
    "Unfortunately, the implementations are better seen in R or using statsmodel library in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Odds\n",
    "\n",
    "The **probability** that an event will occur is the fraction of times you expect to see that event in many trials. \n",
    "Probabilities always range between 0 and 1. \n",
    "\n",
    "The **odds** are defined as the probability that the event will occur divided by the probability that the event will not occur.  \n",
    "$\\frac{y}{(1-y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* If a race horse runs 100 races and wins 25 times and loses the other 75 times, the probability of winning is 25/100 = 0.25 or 25%. The odds of the horse winning are 25/75 = 0.333 or 1 win to 3 loses.\n",
    "\n",
    "* If the horse runs 100 races and wins 5 and loses the other 95 times, the probability of winning is 0.05 or 5%, and the odds of the horse winning are 5/95 = 0.0526.\n",
    "\n",
    "* If the horse runs 100 races and wins 50, the probability of winning is 50/100 = 0.50 or 50%, and the odds of winning are 50/50 = 1 (even odds).\n",
    "\n",
    "* If the horse runs 100 races and wins 80, the probability of winning is 80/100 = 0.80 or 80%, and the odds of winning are 80/20 = 4 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What would happen if the horse is too bad at racing? What would happen if the horse is the best? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If the horse is too bad, the odds of winning will tend to zero.\n",
    "\n",
    "But if the horse is too good, the odds of winning will be too high.... they can reach infinity... \n",
    "\n",
    "How can we compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Log Odds**  \n",
    "So now that we understand Odds and Probability, we can understand Log Odds. \n",
    "\n",
    "Log of Odds is nothing but log(odds). Log Odds makes the magnitude of odds against look so much smaller to those in favor.\n",
    "\n",
    "When we take a log of odds, we make it look symmetrical.\n",
    "\n",
    "$Logit Function = log\\frac{p}{1-p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Odds of winning = 4/6 = 0.6666  \n",
    "log(Odds of winning) = log(0.6666) = -0.176  \n",
    "Odds of losing = 6/4 = 1.5  \n",
    "log(Odds of losing) = log(1.5) = 0.176  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The value of the logit function heads towards infinity as p approaches 1 and towards negative infinity as it approaches 0.\n",
    "\n",
    "The logit function is useful in analytics because it maps probabilities (which are values in the range [0, 1]) to the full range of real numbers. In particular, if you are working with “yes-no” (binary) inputs it can be useful to transform them into real-valued quantities prior to modeling. This is essentially what happens in logistic regression.\n",
    "\n",
    "The inverse of the logit function is the sigmoid function. That is, if you have a probability p, sigmoid(logit(p)) = p. The sigmoid function maps arbitrary real values back to the range [0, 1]. \n",
    "\n",
    "The larger the value, the closer to 1 you’ll get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The formula for the sigmoid function is $\\sigma(x) = \\frac{1}{(1 + exp^{-x})}$ . \n",
    "\n",
    "Here is a plot of the function:\n",
    "\n",
    "![sigmoid](imgs/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The sigmoid function is what we associate as the Logistic Regression. The sigmoid might be useful if you want to transform a real valued variable into something that represents a probability. This sometimes happens at the end of a classification process. \n",
    "\n",
    "There are other functions that map probabilities to reals (and vice-versa), so what is special about the logit and sigmoid? \n",
    "- The logit function has the nice connection to odds described at the beginning. Even if we associate LogReg with the squiggly line, we CANNOT forget that the COEFFICIENTS ARE presented in terms of the log(odds) graph.\n",
    "\n",
    "- Gradients of the logit and sigmoid are simple to calculate. Many optimization and ML techniques use gradients.\n",
    "\n",
    "The biggest drawback of the sigmoid function is the so-called “vanishing gradient” problem (the gradient is vanishingly small, preventing the weight from changing its value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
